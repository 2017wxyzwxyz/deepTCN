{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.read_csv(\"traffic.csv\")\n",
    "traffic = traffic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic=traffic.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(963, 10560)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateList = pd.date_range(start = '01/01/2008', end='30/03/2009')\n",
    "dateRemove  = pd.to_datetime(['2008-01-01', '2008-01-21', '2008-02-18','2008-03-31','2008-03-09', '2008-05-26','2008-07-04','2008-09-01','2008-11-11','2008-11-17', '2008-12-25', '2009-01-01','2009-01-21','2009-02-16',\"2009-03-08\"])\n",
    "dateList = [s for s in dateList if s not in dateRemove]\n",
    "\n",
    "hour_list = []\n",
    "for nDate in dateList:\n",
    "    for nHour in range(24):\n",
    "        tmp_timestamp = nDate+timedelta(hours=nHour)\n",
    "        hour_list.append(tmp_timestamp)\n",
    "hour_list = np.array(hour_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10560,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_index = list(range(963))\n",
    "\n",
    "moving_window_dis = 24;\n",
    "sample_len = 192; #168+24\n",
    "input_len = 168;\n",
    "output_len = 24;\n",
    "total_n = 440-7; ## The total days\n",
    "test_n = 7     ## The testing days, day of the last 7 days\n",
    "train_n = total_n - test_n ## The training days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_list = [];trainY_list = [];train_X2_list = []\n",
    "testX_list = []; testY_list = []; test_X2_list = []\n",
    "#testX_list = [];testX2_list = [];testY_list = [];testY2_list = []\n",
    "#for station in station_index:\n",
    "for station in station_index:\n",
    "    sub_series = traffic[station,:] \n",
    "\n",
    "    trainX = np.zeros(shape=(train_n, input_len))       ## The input series\n",
    "    trainY = np.zeros(shape=(train_n, output_len))      ## The output series  \n",
    "\n",
    "    testX  = np.zeros(shape=(test_n, input_len))        ## The input series\n",
    "    testY = np.zeros(shape=(test_n, output_len))        ## The output series\n",
    "\n",
    "    covariate_num = 6   # other features covariate_num: station_id,nYear,nMonth,day_of_month, day_of_week, iHour\n",
    "    trainX2 = np.zeros(shape=(train_n, sample_len,covariate_num))\n",
    "    testX2 = np.zeros(shape=(test_n, sample_len,covariate_num))\n",
    "    ### Testing samples (7+1)*24\n",
    "    ts_len = sub_series.shape[0]\n",
    "    start_index = ts_len-sample_len\n",
    "    for i in range(total_n):\n",
    "        ### The sequence data\n",
    "        series_x = sub_series[start_index:start_index+input_len]    #168\n",
    "        series_y = sub_series[start_index+input_len:start_index+sample_len]  #24\n",
    "        ### The covariate\n",
    "        station_XY = np.repeat(station, sample_len)\n",
    "        ### the time index\n",
    "        time_index_xy = pd.to_datetime(hour_list[start_index:start_index+sample_len])\n",
    "        #print(time_index_xy)\n",
    "        nYear_XY = time_index_xy.year-2008\n",
    "        nMonth_XY = time_index_xy.month-1\n",
    "        mDay_XY = time_index_xy.day-1\n",
    "        wDay_XY = time_index_xy.weekday\n",
    "        nHour_XY = time_index_xy.hour\n",
    "        #print(station_XY.shape,nYear_XY.shape,nMonth_XY.shape,mDay_XY.shape,wDay_XY.shape,nHour_XY.shape)\n",
    "        covariate_XY = np.c_[station_XY,nYear_XY,nMonth_XY,mDay_XY,wDay_XY,nHour_XY]  #192*6\n",
    "        \n",
    "        if(i<test_n):\n",
    "            testX[i] = series_x\n",
    "            testY[i] = series_y\n",
    "            testX2[i,:,:] = covariate_XY\n",
    "    \n",
    "        else:\n",
    "            trainX[i-test_n] = series_x\n",
    "            trainY[i-test_n] = series_y\n",
    "            trainX2[i-test_n] = covariate_XY\n",
    "        # update the start_index\n",
    "        start_index = start_index - moving_window_dis\n",
    "    \n",
    "    testX_list.append(testX)\n",
    "    test_X2_list.append(testX2)\n",
    "    testY_list.append(testY)\n",
    "    #print(np.array(test_X2_list).shape)\n",
    "    trainX_list.append(trainX)\n",
    "    train_X2_list.append(trainX2)\n",
    "    trainY_list.append(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_dt = np.vstack(trainX_list)\n",
    "trainY_dt = np.vstack(trainY_list)\n",
    "train_X2_dt = np.vstack(train_X2_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_dt = np.vstack(testX_list)\n",
    "testY_dt = np.vstack(testY_list)\n",
    "test_X2_dt = np.vstack(test_X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410238, 168)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410238, 192, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X2_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410238, 24)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6741, 168)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6741, 24)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6741, 192, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X2_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the data\n",
    "with open('tensor_prepare.pkl', 'wb') as f:\n",
    "    pickle.dump([trainX_dt,train_X2_dt, trainY_dt,testX_dt, test_X2_dt,testY_dt], f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=np.concatenate((trainX_dt,testX_dt), axis=0)\n",
    "#print(alldata)\n",
    "\n",
    "alldata_scaled= preprocessing.scale(alldata)\n",
    "\n",
    "trainX_dt=alldata_scaled[0:trainX_dt.shape[0]]\n",
    "testX_dt=alldata_scaled[trainX_dt.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the data\n",
    "with open('tensor_prepare_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump([trainX_dt,train_X2_dt, trainY_dt,testX_dt, test_X2_dt,testY_dt], f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
